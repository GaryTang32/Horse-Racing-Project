{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import DateType\n",
    "import os\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n",
      "Dropped 99,47 and other Place values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gt/spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/home/gt/spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/gt/spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 642, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/gt/spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gt/spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 186, in manager\n",
      "  File \"/home/gt/spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/home/gt/spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 642, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "  File \"/home/gt/spark-3.0.3-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 595, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----+------------+--------+----------+-----------------+------+---------------+----+----------------+--------+----------+---------------+---------------+---------------+---------------+-----------+---------+--------+----------+--------+------------------------+\n",
      "|Record_ID|Race_ID|Place|Horse_Number|Horse_ID|Horse_Code|       Horse_Name|Weight|Weight_Declared|Draw|Distance_Between|Win_odds|Place_odds|Place_Section_1|Place_Section_2|Place_Section_3|Place_Section_4|Finish_time|Jockey_ID|  Jockey|Trainer_ID| Trainer|Finish_time_mille_second|\n",
      "+---------+-------+-----+------------+--------+----------+-----------------+------+---------------+----+----------------+--------+----------+---------------+---------------+---------------+---------------+-----------+---------+--------+----------+--------+------------------------+\n",
      "|        2|      1|    1|           5|    1628|      B114|  SHANGHAI DRAGON|   130|           1186|   5|               -|     6.6|       2.4|              1|              1|              1|              1|    1:36.14|        1|K Teetan|         1|T P Yung|                   9.614|\n",
      "|        3|      1|    2|          12|    1023|      C261|MANNA FROM HEAVEN|   119|           1107|   7|             3/4|     224|     56.75|              5|              5|              5|              2|    1:36.28|        2|  C Wong|         2| K L Man|                   9.628|\n",
      "+---------+-------+-----+------------+--------+----------+-----------------+------+---------------+----+----------------+--------+----------+---------------+---------------+---------------+---------------+-----------+---------+--------+----------+--------+------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Cast Date\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-----+----------------+------+---------------+----+----------------+--------+----------+---------------+---------------+---------------+---------------+-----------+---------+--------+----------+-------+------------------------+------------+---------+----------+-------------+-------+-------+------------+------+--------+---------+---------------+-----------+\n",
      "|Horse_ID|Race_ID|Place|      Horse_Name|Weight|Weight_Declared|Draw|Distance_Between|Win_odds|Place_odds|Place_Section_1|Place_Section_2|Place_Section_3|Place_Section_4|Finish_time|Jockey_ID|  Jockey|Trainer_ID|Trainer|Finish_time_mille_second|      Course|    Prize|      Date|Distance_Type|  Class|Ranking|Surface_Type| State|     Sex|Foal_Date|First_Race_Date|Age_At_Race|\n",
      "+--------+-------+-----+----------------+------+---------------+----+----------------+--------+----------+---------------+---------------+---------------+---------------+-----------+---------+--------+----------+-------+------------------------+------------+---------+----------+-------------+-------+-------+------------+------+--------+---------+---------------+-----------+\n",
      "|     148|     32|   12|FAITHFUL TRINITY|   127|           1098|   3|              ML|     4.7|     1.925|              4|              6|              9|             12|    2:00.03|        6|Z Purton|        10| W Y So|                  12.003|Happy Valley|1,000,000|2021-01-06|       Medium|Class 4|  60-40|        Turf|Active| Gelding| 9/3/2014|     2017-01-09|          6|\n",
      "|     148|     84|    4|FAITHFUL TRINITY|   128|           1100|   8|           2-3/4|     4.6|       1.9|              1|              2|              2|              4|    1:40.43|        6|Z Purton|        10| W Y So|                  10.043|Happy Valley|1,000,000|2020-01-16|       Medium|Class 4|  60-40|        Turf|Active| Gelding| 9/3/2014|     2017-01-09|          6|\n",
      "+--------+-------+-----+----------------+------+---------------+----+----------------+--------+----------+---------------+---------------+---------------+---------------+-----------+---------+--------+----------+-------+------------------------+------------+---------+----------+-------------+-------+-------+------------+------+--------+---------+---------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "None\n",
      "Age at race done\n",
      "Win odds calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage and place percentage calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55404\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date, Weight: float, Age_At_Race: bigint, Win_odds: float, Win_odds_%: double, Draw: float, Prize: int, Surface_Type: string, Distance_Type: string, Class: string, Ranking: string, Course: string, Sex: string, MaxTemp: double, MinTemp: double, jockey_first_place_ratio: double, jockey_second_place_ratio: double, jockey_third_place_ratio: double, jockey_place_ratio: double, trainer_first_place_ratio: double, trainer_second_place_ratio: double, trainer_third_place_ratio: double, trainer_place_ratio: double, Win_Perc: float, Place_Perc: float, Target: float, Sex_index: double, Class_index: double, Ranking_index: double, Distance_Type_index: double, Surface_Type_index: double, Course_index: double]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#import data\n",
    "def read_data(spark):\n",
    "    directory_path1 = os.path.join(os.getcwd(),\"Full_Data_Pack_1\")\n",
    "    directory_path2 = os.path.join(os.getcwd(),\"Full_Data_Pack_2\")\n",
    "\n",
    "    df_horse = spark.read.csv(os.path.join(\"horses.csv\"), header=True, inferSchema=True)\n",
    "    df_jockey = spark.read.csv(os.path.join(\"jockeys.csv\"), header=True, inferSchema=True)\n",
    "    df_races_sectional = spark.read.csv(os.path.join(\"races_sectional.csv\"), header=True, inferSchema=True)\n",
    "    df_trainer = spark.read.csv(os.path.join(\"trainer.csv\"), header=True, inferSchema=True)\n",
    "    df_sectional = spark.read.csv(os.path.join(\"sectional_table.csv\"), header=True, inferSchema=True)\n",
    "    df_records = spark.read.csv(os.path.join(\"records.csv\"), header=True, inferSchema=True)\n",
    "    df_races = spark.read.csv(os.path.join(\"races.csv\"), header=True, inferSchema=True)\n",
    "    df_foal = spark.read.csv(os.path.join(\"foal_date.csv\"), header=True, inferSchema=True)\n",
    "    \n",
    "    return df_races, df_races_sectional, df_trainer, df_jockey, df_records, df_horse, df_sectional,df_foal\n",
    "\n",
    "#trainer preprocessing\n",
    "def trainer_preprocessing(df):\n",
    "    df = df.withColumn('Total_wins',when(df[\"Total_wins\"].isNull(),0).\\\n",
    "                      otherwise(df[\"Total_wins\"]))\n",
    "    df = df.withColumn('Total_second_places',when(df[\"Total_second_places\"].isNull(),0).\\\n",
    "                      otherwise(df[\"Total_second_places\"]))\n",
    "    df = df.withColumn('Total_third_places',when(df[\"Total_third_places\"].isNull(),0).\\\n",
    "                      otherwise(df[\"Total_third_places\"]))\n",
    "    df = df.select(\"Trainer_ID\",\"Total_wins\",\"Total_second_places\",\"Total_third_places\",\"Total_rides\")\n",
    "    df = df.select(\"Trainer_ID\",(col(\"Total_wins\")/col(\"Total_rides\")).alias(\"trainer_first_place_ratio\"),\\\n",
    "                                (col(\"Total_second_places\")/col(\"Total_rides\")).alias(\"trainer_second_place_ratio\"),\\\n",
    "                                (col(\"Total_third_places\")/col(\"Total_rides\")).alias(\"trainer_third_place_ratio\"),\\\n",
    "                                ((col(\"Total_wins\") + col(\"Total_second_places\") + col(\"Total_third_places\"))/col(\"Total_rides\")).alias(\"trainer_place_ratio\"),\\\n",
    "                                ((col(\"Total_rides\") - (col(\"Total_wins\") + col(\"Total_second_places\") + col(\"Total_third_places\")))/col(\"Total_rides\")).alias(\"trainer_lose_ratio\"))\n",
    "    return df\n",
    "\n",
    "#jockey preprocessing\n",
    "def jockey_preprocessing(df):\n",
    "    df = df.withColumn('Total_wins',when(df[\"Total_wins\"].isNull(),0).\\\n",
    "                      otherwise(df[\"Total_wins\"]))\n",
    "    df = df.withColumn('Total_second_places',when(df[\"Total_second_places\"].isNull(),0).\\\n",
    "                      otherwise(df[\"Total_second_places\"]))\n",
    "    df = df.withColumn('Total_third_places',when(df[\"Total_third_places\"].isNull(),0).\\\n",
    "                      otherwise(df[\"Total_third_places\"]))\n",
    "    df = df.select(\"Jockey_ID\",\"Total_wins\",\"Total_second_places\",\"Total_third_places\",\"Total_rides\")\n",
    "    df = df.select(\"Jockey_ID\",(col(\"Total_wins\")/col(\"Total_rides\")).alias(\"jockey_first_place_ratio\"),\\\n",
    "                                (col(\"Total_second_places\")/col(\"Total_rides\")).alias(\"jockey_second_place_ratio\"),\\\n",
    "                                (col(\"Total_third_places\")/col(\"Total_rides\")).alias(\"jockey_third_place_ratio\"),\\\n",
    "                                ((col(\"Total_wins\") + col(\"Total_second_places\") + col(\"Total_third_places\"))/col(\"Total_rides\")).alias(\"jockey_place_ratio\"),\\\n",
    "                                ((col(\"Total_rides\") - (col(\"Total_wins\") + col(\"Total_second_places\") + col(\"Total_third_places\")))/col(\"Total_rides\")).alias(\"jockey_lose_ratio\"))\n",
    "    return df\n",
    "\n",
    "year_threshold = 2015\n",
    "#race_preprocessing\n",
    "def race_preprocessing(df):\n",
    "    \n",
    "    def return_year(x):\n",
    "        return int(str(x)[:4])\n",
    "    \n",
    "    class_trans_dict = {\n",
    "        'Hong Kong Group One': 'Group One',\n",
    "        'Hong Kong Group Three': 'Group Three',\n",
    "        'Group One': 'Group 1',\n",
    "        'Class 4 (Special Condition)': 'Class 4',\n",
    "        'Hong Kong Group Two': 'Group Two',\n",
    "        'Class 4 (Restricted)': 'Class 4',\n",
    "        'Class 3 (Special Condition)': 'Group 1',\n",
    "        'Class 2 (Bonus Prize Money)': 'Class 2',\n",
    "        'Class 3 (Bonus Prize Money)': 'Class 3',\n",
    "        'Class 4 (Bonus Prize Money)': 'Class 4',\n",
    "        '4 Year Olds ': '4 Year Olds',\n",
    "        'Restricted Race': 'Griffin Race'}\n",
    "    \n",
    "    def map_race_class(x):\n",
    "        if x in class_trans_dict.keys():\n",
    "            return class_trans_dict[x]\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def fix_surface_type(x):\n",
    "        if 'TURF' in str(x):\n",
    "            return 'Turf'\n",
    "        else:\n",
    "            return 'All_Weather'\n",
    "        \n",
    "    returnyear_func = udf(return_year,IntegerType())\n",
    "    map_race_class_func = udf(map_race_class,StringType())\n",
    "    fix_surface_type_func = udf(fix_surface_type,StringType())\n",
    "    \n",
    "    df = df.withColumn(\"Year\",returnyear_func(df[\"Date\"]))\n",
    "    df = df.select('*').where(f'Year > {year_threshold}')\n",
    "    \n",
    "    #getting distinct distance values\n",
    "    distance_list = df.select(\"Distance\").distinct().orderBy(\"Distance\").rdd.map(lambda x:x.Distance).collect()\n",
    "    \n",
    "    #map class based on mapping dictionary\n",
    "    df = df.withColumn(\"Class\",map_race_class_func(df[\"Class\"]))\n",
    "    #Concatenate Class and Ranking\n",
    "    df = df.select('*',concat_ws(\"_\",\"Class\",\"Ranking\"))\\\n",
    "           .withColumnRenamed('concat_ws(_, Class, Ranking)','class_rank')\n",
    "    #set Surface Type\n",
    "    df = df.withColumn(\"Surface_Type\",fix_surface_type_func(df[\"Surface\"]))\n",
    "    df = df.select('*',lit('Short').alias('Distance_Type'))\n",
    "    df = df.withColumn(\"Distance_Type\",when(((df[\"Course\"] == 'Sha Tin') & (df[\"Surface_Type\"] == \"Turf\")\\\n",
    "                  & (df[\"Distance\"]  > 1400) & (df['Distance'] <= 1800)),\"Medium\").\\\n",
    "                       otherwise(df[\"Distance_Type\"]))\n",
    "    df = df.withColumn(\"Distance_Type\",when(((df[\"Course\"] == 'Sha Tin') & (df[\"Surface_Type\"] == \"Turf\")\\\n",
    "              & (df['Distance'] > 1800)),\"Long\").\\\n",
    "                   otherwise(df[\"Distance_Type\"]))\n",
    "    df = df.withColumn(\"Distance_Type\",when(((df[\"Course\"] == 'Sha Tin') & (df[\"Surface_Type\"] == \"All_Weather\")\\\n",
    "              & (df['Distance'] > 1300)),\"Medium\").\\\n",
    "                   otherwise(df[\"Distance_Type\"]))    \n",
    "    df = df.withColumn(\"Distance_Type\",when(((df[\"Course\"] == 'Happy Valley')\\\n",
    "                  & (df[\"Distance\"]  > 1200) & (df['Distance'] <= 1800)),\"Medium\").\\\n",
    "                       otherwise(df[\"Distance_Type\"]))\n",
    "    df = df.withColumn(\"Distance_Type\",when(((df[\"Course\"] == 'Happy Valley')\\\n",
    "                  & (df[\"Distance\"]  > 1800)),\"Long\").\\\n",
    "                       otherwise(df[\"Distance_Type\"]))\n",
    "    return df\n",
    "\n",
    "#record preprocessing\n",
    "\n",
    "def record_preprocessing(df,df_races,df_horses):\n",
    "    def parse_placings(x):\n",
    "        return int(x.split(\" \")[0])\n",
    "    \n",
    "#     def parse_finish_time(x):\n",
    "#         print(str(x))\n",
    "#         x = str(x)\n",
    "#         if ':' in x:\n",
    "#             time = int(x.split(\":\")[0]) * 60 * 100 + int(x.split(\":\")[1].split(\".\")[0]) * 100 + int(x.split(\":\")[1].split(\".\")[1])\n",
    "#         else :\n",
    "# #             print(x)\n",
    "#             time = int(x.split(\".\")[0]) * 100 + int(x.split(\".\")[1])\n",
    "#         return time#     \n",
    "\n",
    "    def parse_finish_time(x):\n",
    "#         print(str(x))\n",
    "        time = int(x[0]) * 60 * 100 + int(x[2:4]) * 100 + int(x[5:]) \n",
    "        time = time / 1000\n",
    "        return time\n",
    "    \n",
    "    splitfunc = udf(parse_placings,IntegerType())\n",
    "    convert_time = udf(parse_finish_time,FloatType())\n",
    "    \n",
    "    print('Start')\n",
    "    df = df.select('*').where(\"Place != 'DISQ' AND Place != 'DNF' AND Place != 'FE' AND Place != 'PU' AND Place != 'TNP' AND Place != 'UR' AND Place != 'VOID' AND Place != 'WR' AND Place != 'WV' AND Place != 'WV-A' AND Place != 'WX' AND Place != 'WX-A' AND Place != 'WXNR' AND Place IS NOT NULL\")\n",
    "    df = df.withColumn(\"Place\",splitfunc(df[\"Place\"]))\n",
    "    df = df.select('*').where('Place != 99 AND Place != 47')\n",
    "    print('Dropped 99,47 and other Place values')\n",
    "    \n",
    "    df = df.select('*').where('Finish_time is not null').where('Place_Section_1 is not null')\n",
    "    df = df.withColumn(\"Finish_time_mille_second\", convert_time(df['Finish_time']))\n",
    "    df.show(2)\n",
    "    \n",
    "    df = df.drop('Record_ID','Horse_Number','Horse_Code')\n",
    "    df = df.withColumn('Win_odds',col('Win_odds').cast(FloatType()))\n",
    "    df = df.join(df_races.select('Race_ID','Course','Prize','Date',\"Distance_Type\",\"Class\",\"Ranking\",\"Surface_Type\"),'Race_ID')\n",
    "    df = df.join(df_horses.select(\"Horse_ID\",\"Age\",\"State\",\"Sex\",\"Foal_Date\"),\"Horse_ID\")\n",
    "    spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "    df = df.withColumn('Date',to_date(col('Date'),\"yyyy-mm-dd\"))\n",
    "    print('Cast Date')\n",
    "    df = df.drop('Age')\n",
    "    first_race_date_df = df.select('Horse_ID','Date').\\\n",
    "                    groupby('Horse_ID').agg(min('Date')).\\\n",
    "                    withColumnRenamed('min(Date)','First_Race_Date').\\\n",
    "                    orderBy('Horse_ID')\n",
    "    df = df.join(first_race_date_df,\"Horse_ID\")\n",
    "    \n",
    "    #can be changed with average of foal date\n",
    "    #use a table with foal date and age_at_first_race by using the first_race_date_df\n",
    "    #floor(datediff(col(\"First_race_date_df\"),col(\"Foal Date\"))/365)\n",
    "    #join the above with df\n",
    "    #fill the missing values with the mean value for age_at_first_race\n",
    "    df = df.select('*',(3 + floor(datediff(col(\"Date\"),col(\"First_Race_Date\"))/365)).\\\n",
    "                       alias('Age_At_Race'))\n",
    "    \n",
    "    print(df.show(2))\n",
    "    #making Win Odds into a value between 0 and 1\n",
    "    df_sum_win_odds_reciprocal = df.select(\"Race_ID\",\"Horse_ID\",\"Win_odds\",\"Prize\",(1/col(\"Win_odds\")).alias('Reciprocal Win Odds'))\\\n",
    "                     .groupBy(\"Race_ID\").sum(\"Reciprocal Win Odds\")\\\n",
    "                     .withColumnRenamed('sum(Reciprocal Win Odds)','Sum Reciprocal')\\\n",
    "                     .orderBy(\"Race_ID\")\n",
    "    \n",
    "    #As we have the sum of reciprocal of the Win Odds of each race, we can divide\n",
    "    #the price money by this sum, to get the money available after HKJC takes \n",
    "    #it's commission\n",
    "    print('Age at race done')\n",
    "    df = df.withColumn(\"Prize\",regexp_replace(\"Prize\",\",\",\"\").cast(IntegerType()))\\\n",
    "                     .select('Race_ID',\"Horse_ID\",\"Weight\",\"Weight_Declared\",\\\n",
    "                            \"Win_odds\",\"Draw\",\"Place\",\"Prize\",\"Course\",\"Surface_Type\",\"Distance_Type\",\"Class\",\"Ranking\",\"Date\",\"State\",\"Sex\",\"First_Race_Date\",\\\n",
    "                            \"Age_At_Race\",\"Jockey_ID\",\"Trainer_ID\", \"Finish_time_mille_second\")\\\n",
    "                     .join(df_sum_win_odds_reciprocal,\"Race_ID\")\\\n",
    "                     .select('Race_ID',\"Horse_ID\",\"Weight\",\"Weight_Declared\",\\\n",
    "                            \"Win_odds\",\"Draw\",\"Place\",\"Prize\",\"Course\",\"Surface_Type\",\"Distance_Type\",\"Class\",\"Ranking\",\"Date\",\"State\",\"Sex\",\"First_Race_Date\",\\\n",
    "                            \"Age_At_Race\",\"Jockey_ID\",\"Trainer_ID\",(col(\"Prize\")/col(\"Sum Reciprocal\")).alias(\"Available Prize Money\"), \"Finish_time_mille_second\")\\\n",
    "                     .select('Race_ID',\"Horse_ID\",\"Weight\",\"Weight_Declared\",\\\n",
    "                            \"Win_odds\",\"Draw\",\"Place\",\"Prize\",\"Course\",\"Surface_Type\",\"Distance_Type\",\"Class\",\"Ranking\",\"Date\",\"State\",\"Sex\",\"First_Race_Date\",\\\n",
    "                            \"Age_At_Race\",\"Jockey_ID\",\"Trainer_ID\",((col(\"Available Prize Money\")/col(\"Win_odds\"))/col(\"Prize\")).alias(\"Win_odds_%\"),\"Finish_time_mille_second\")\\\n",
    "                     .orderBy(\"Race_ID\")\n",
    "    print('Win odds calculated')\n",
    "    #drop weight_declared as it has too many missing values\n",
    "    df = df.drop('Weight_Declared')\n",
    "    #As Weight declared has only 12 pieces of data with '---', we drop these too\n",
    "    df = df.select(\"*\").where(\"Weight != '---'\")\n",
    "    \n",
    "    #There are races with only 1 to 4 competitors. These will be dropped\n",
    "    df_low_placings = None\n",
    "    for i in range(1,5):\n",
    "        if df_low_placings == None:\n",
    "            df_low_placings = df.select(\"Race_ID\",\"Place\")\\\n",
    "                             .groupby(\"Race_ID\")\\\n",
    "                             .agg(max(\"Place\"))\\\n",
    "                             .withColumnRenamed(\"max(Place)\",\"Place\")\\\n",
    "                             .select(\"Race_ID\",\"Place\")\\\n",
    "                             .where(f\"Place = {i}\")\n",
    "            df_low_placings.cache()\n",
    "        else:\n",
    "            df_low_placings = df_low_placings.union(df.select(\"Race_ID\",\"Place\")\\\n",
    "                             .groupby(\"Race_ID\")\\\n",
    "                             .agg(max(\"Place\"))\\\n",
    "                             .withColumnRenamed(\"max(Place)\",\"Place\")\\\n",
    "                             .select(\"Race_ID\",\"Place\")\\\n",
    "                             .where(f\"Place = {i}\"))\n",
    "            df_low_placings.cache()\n",
    "    #list of race ids with only 1 to 4 competitors\n",
    "    race_id_list = df_low_placings.select(\"Race_ID\").rdd.map(lambda x:x.Race_ID).collect()\n",
    "    df = df.select('Race_ID',\"Horse_ID\",\"Weight\",\"Age_At_Race\",\\\n",
    "                            \"Win_odds\",\"Win_odds_%\",\"Draw\",\"Place\",\"Prize\",\"Surface_Type\",\"Distance_Type\",\"Class\",\"Ranking\",\"Course\",\"Date\",\"State\",\"Sex\",\"First_Race_Date\",\"Jockey_ID\",\"Trainer_ID\",\"Finish_time_mille_second\")\\\n",
    "                            .where(~col(\"Race_ID\").isin(race_id_list))\n",
    "    return df\n",
    "\n",
    "#horse_preprocessing\n",
    "def horse_preprocessing(df):\n",
    "    df = df.select('Horse_ID', 'State', 'Country', 'Age', 'Color', 'Sex', 'Import_type', 'Total_Stakes', 'Last_Rating')\n",
    "    return df\n",
    "\n",
    "#sectional_preprocessing\n",
    "def sectional_preprocessing(df):\n",
    "    df = df.select('Race_ID', 'Horse_ID', 'Finish_time')\n",
    "    return df\n",
    "\n",
    "def foal_preprocessing(df_horse,df_foal):\n",
    "    df = df_horse.join(df_foal.select('Horse_ID','Foal_Date'),'Horse_ID','left')\n",
    "    return df\n",
    "\n",
    "def calculate_win_percentage(partition):\n",
    "    for horse in partition:\n",
    "        horse_id = horse[0]\n",
    "        win_count = 0\n",
    "        total_count = 0\n",
    "        win_percentage = list()\n",
    "        for i in horse[1]:\n",
    "            total_count += 1\n",
    "            #i is a tuple having race_id,data,place\n",
    "            if i[2] == 1:\n",
    "                win_count += 1\n",
    "            win_percentage.append((horse_id,i[0],i[1],(win_count/total_count) * 100))\n",
    "            \n",
    "            \n",
    "        yield (horse_id,win_percentage)\n",
    "\n",
    "def calculate_place_percentage(partition):\n",
    "    for horse in partition:\n",
    "        horse_id = horse[0]\n",
    "        place_count = 0\n",
    "        total_count = 0\n",
    "        place_percentage = list()\n",
    "        for i in horse[1]:\n",
    "            total_count += 1\n",
    "            #i is a tuple having race_id,data,place\n",
    "            if (i[2] == 1) or (i[2] == 2) or (i[2] == 3):\n",
    "                place_count += 1\n",
    "            place_percentage.append((horse_id,i[0],i[1],(place_count/total_count) * 100))\n",
    "            \n",
    "        yield (horse_id,place_percentage)\n",
    "\n",
    "#Example of Divide and Conquer being used\n",
    "#pass in the records df to this function\n",
    "def get_win_and_place_percentage_df(df,spark):\n",
    "    sc = spark.sparkContext\n",
    "    df_horse_place = df.select('Race_ID','Horse_ID','Date','Place').\\\n",
    "        groupby('Horse_ID','Race_ID','Date').agg(max(col('Place'))).\\\n",
    "        withColumnRenamed('max(Place)','Place').\\\n",
    "        orderBy('Horse_ID','Date')\n",
    "    #difficult to apply pandas type operations on sparksql\n",
    "    #requires pyarrow which isnt installing \n",
    "    #Turn to RDD and use Divide and conquer\n",
    "    horse_place_rdd = df_horse_place.rdd\n",
    "    horse_place_rdd = horse_place_rdd.map(lambda x: (x.Horse_ID,(x.Race_ID,x.Date,x.Place)))\n",
    "    #Group by key to get all races that a horse has participated in\n",
    "    #Key is horse ID\n",
    "    #Make the values to a list format while maintaining the partitioning\n",
    "    #that we get by groupByKey by using mapValues\n",
    "    grouped_horse_id_rdd = horse_place_rdd.groupByKey().mapValues(list)\n",
    "    #apply the mapPartitions method to do D&C\n",
    "    win_percent = grouped_horse_id_rdd.mapPartitions(calculate_win_percentage)\n",
    "    place_percent = grouped_horse_id_rdd.mapPartitions(calculate_place_percentage)\n",
    "    #result is mapped to get only the values from the key,value pair\n",
    "    #then we flatMap it to get to rdd format for dataframe\n",
    "    win_percent_rdd = win_percent.map(lambda x: x[1]).flatMap(lambda x:x)\n",
    "    place_percent_rdd = place_percent.map(lambda x:x[1]).flatMap(lambda x:x)\n",
    "    schema_win_percent  = StructType([\n",
    "    StructField(\"Horse_ID\",IntegerType(),True),\n",
    "    StructField(\"Race_ID\",IntegerType(),True),\n",
    "    StructField(\"Date\",DateType(),True),\n",
    "    StructField(\"Win_Perc\",FloatType(),True)\n",
    "    ])\n",
    "\n",
    "    win_percent_dataframe = spark.createDataFrame(win_percent_rdd,schema_win_percent)\n",
    "\n",
    "    schema_place_percent  = StructType([\n",
    "    StructField(\"Horse_ID\",IntegerType(),True),\n",
    "    StructField(\"Race_ID\",IntegerType(),True),\n",
    "    StructField(\"Date\",DateType(),True),\n",
    "    StructField(\"Place_Perc\",FloatType(),True)\n",
    "    ])\n",
    "\n",
    "    place_percent_dataframe = spark.createDataFrame(place_percent_rdd,schema_place_percent)\n",
    "\n",
    "    return win_percent_dataframe,place_percent_dataframe\n",
    "\n",
    "# Get wather data from MongoDB\n",
    "# def get_weather_data(spark):\n",
    "#     pipeline1 = \"{'$project': {'day': 1,'month':1,'year':1,'sha_tin_max':1,'sha_tin_min':1,'_id':0}}\"\n",
    "#     pipeline2 = \"{'$project': {'day': 1,'month':1,'year':1,'happy_velley_max':1,'happy_velley_min':1,'_id':0}}\"\n",
    "#     df1 = spark.read.format(\"mongo\").option('pipeline',pipeline1).load()\n",
    "#     df2 = spark.read.format(\"mongo\").option('pipeline',pipeline2).load()\n",
    "#     df1 = df1.select('*',lit('Sha Tin').alias('Course'))\\\n",
    "#              .withColumnRenamed('sha_tin_max','MaxTemp')\\\n",
    "#              .withColumnRenamed('sha_tin_min','MinTemp')\n",
    "#     df2 = df2.select('*',lit('Happy Valley').alias('Course'))\\\n",
    "#              .withColumnRenamed('happy_velley_max','MaxTemp')\\\n",
    "#              .withColumnRenamed('happy_velley_min','MinTemp')\n",
    "#     df1 = df1.select(concat_ws('/',df1.month.cast(IntegerType()),df1.day.cast(IntegerType()),df1.year.cast(IntegerType())).alias('Date'),'Course','MaxTemp','MinTemp')\n",
    "#     df2 = df2.select(concat_ws('/',df2.month.cast(IntegerType()),df2.day.cast(IntegerType()),df2.year.cast(IntegerType())).alias('Date'),'Course','MaxTemp','MinTemp')\n",
    "#     spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "#     df1 = df1.withColumn('Date',to_date(col('Date'),\"M/dd/yyyy\"))\n",
    "#     df2 = df2.withColumn('Date',to_date(col('Date'),\"M/dd/yyyy\"))\n",
    "#     return df1.union(df2)\n",
    "\n",
    "# Get weather_data in \n",
    "def get_weather_data(spark):\n",
    "#     pipeline1 = \"{'$project': {'day': 1,'month':1,'year':1,'sha_tin_max':1,'sha_tin_min':1,'_id':0}}\"\n",
    "#     pipeline2 = \"{'$project': {'day': 1,'month':1,'year':1,'happy_velley_max':1,'happy_velley_min':1,'_id':0}}\"\n",
    "#     df1 = spark.read.format(\"mongo\").option('pipeline',pipeline1).load()\n",
    "#     df2 = spark.read.format(\"mongo\").option('pipeline',pipeline2).load()\n",
    "    raw = spark.read.csv('weather_data.csv',header=True, inferSchema=True)\n",
    "\n",
    "    \n",
    "    df1 = raw.select('day','month','year','sha_tin_max','sha_tin_min','_c0')\n",
    "    df2 = raw.select('day','month','year',\"happy_velley_max\",'happy_velley_min','_c0')\n",
    "    \n",
    "    \n",
    "    df1 = df1.select('*',lit('Sha Tin').alias('Weather_Course'))\\\n",
    "             .withColumnRenamed('sha_tin_max','MaxTemp')\\\n",
    "             .withColumnRenamed('sha_tin_min','MinTemp')\\\n",
    "             .withColumnRenamed('_c0','_id') \n",
    "    \n",
    "    df2 = df2.select('*',lit('Happy Valley').alias('Weather_Course'))\\\n",
    "             .withColumnRenamed('happy_velley_max','MaxTemp')\\\n",
    "             .withColumnRenamed('happy_velley_min','MinTemp')\\\n",
    "             .withColumnRenamed('_c0','_id')\n",
    "    \n",
    "    df1 = df1.select(concat_ws('/',df1.month.cast(IntegerType()),df1.day.cast(IntegerType()),df1.year.cast(IntegerType())).alias('Weather_Date'),'Weather_Course','MaxTemp','MinTemp')\n",
    "    df2 = df2.select(concat_ws('/',df2.month.cast(IntegerType()),df2.day.cast(IntegerType()),df2.year.cast(IntegerType())).alias('Weather_Date'),'Weather_Course','MaxTemp','MinTemp')\n",
    "    spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "    df1 = df1.withColumn('Weather_Date',to_date(col('Weather_Date'),\"M/dd/yyyy\"))\n",
    "    df2 = df2.withColumn('Weather_Date',to_date(col('Weather_Date'),\"M/dd/yyyy\"))\n",
    "    return df1.union(df2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_races, df_races_sectional, df_trainer, df_jockeys, df_records, df_horse, df_sectional, df_foal = read_data(spark)\n",
    "\n",
    "df_trainer = trainer_preprocessing(df_trainer)\n",
    "df_jockeys = jockey_preprocessing(df_jockeys)\n",
    "df_races = race_preprocessing(df_races)\n",
    "df_horse = horse_preprocessing(df_horse)\n",
    "df_horse = foal_preprocessing(df_horse,df_foal)\n",
    "#note that in records_preprocessing, races and horse are already joined into the dataframe\n",
    "df_records = record_preprocessing(df_records,df_races,df_horse)\n",
    "\n",
    "df_weather = get_weather_data(spark)\n",
    "df_records = df_records.join(df_weather,(df_records[\"Date\"] == df_weather[\"Weather_Date\"]) & (df_records[\"Course\"] == df_weather[\"Weather_Course\"]))\n",
    "\n",
    "df_records_jockey = df_records.join(df_jockeys,\"Jockey_ID\",'left')\n",
    "df_records_jockey_trainer = df_records_jockey.join(df_trainer,\"Trainer_ID\",'left')\n",
    "df_win_percent,df_place_percent = get_win_and_place_percentage_df(df_records,spark)\n",
    "df_records_jockey_trainer_win = df_records_jockey_trainer.join(df_win_percent,['Horse_ID','Race_ID','Date'])\n",
    "print('Win percentage and place percentage calculated')\n",
    "df_records_jockey_trainer_win_place = df_records_jockey_trainer_win.join(df_place_percent,['Horse_ID','Race_ID','Date'])\n",
    "print(df_records_jockey_trainer_win_place.count())\n",
    "print('done')\n",
    "# spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_dataset = df_records_jockey_trainer_win_place.select('Date','Weight','Age_At_Race','Win_odds','Win_odds_%', 'Draw',\\\n",
    "                                                       'Prize','Surface_Type','Distance_Type',\"Class\",\"Ranking\",\\\n",
    "                                                       'Course','Sex','MaxTemp','MinTemp','jockey_first_place_ratio',\\\n",
    "                                                       'jockey_second_place_ratio','jockey_third_place_ratio',\\\n",
    "                                                       'jockey_place_ratio','trainer_first_place_ratio','trainer_second_place_ratio',\n",
    "                                                       'trainer_third_place_ratio','trainer_place_ratio','Win_Perc','Place_Perc',\\\n",
    "                                                        'Finish_time_mille_second' )\n",
    "\n",
    "# df_dataset = df_dataset.withColumn(\"Target\",df_dataset.Weight.cast(FloatType()))\n",
    "df_dataset = df_dataset.withColumn(\"Weight\",df_dataset.Weight.cast(FloatType()))\n",
    "df_dataset = df_dataset.withColumn(\"Draw\",df_dataset.Weight.cast(FloatType()))\n",
    "df_dataset = df_dataset.withColumnRenamed(\"Finish_time_mille_second\", \"Target\")\n",
    "\n",
    "df_dataset = df_dataset.select('*').where('Ranking is not null and Sex is not null and Course is not null and Class is not null and Distance_Type is not null and Surface_Type is not null')\n",
    "\n",
    "features_cols =  [\"Weight\",\"Age_At_Race\",\"Win_odds\",\"Win_odds_%\",\"Draw\",\"Prize\",\"Surface_Type_index\",\"Distance_Type_index\",\n",
    "\"Class_index\",\"Ranking_index\",\"Course_index\",\"Sex_index\",\"MaxTemp\",\"MinTemp\",\"jockey_first_place_ratio\",\"jockey_second_place_ratio\",\n",
    "\"jockey_third_place_ratio\",\"jockey_place_ratio\",\"trainer_first_place_ratio\",\"trainer_second_place_ratio\",\n",
    "\"trainer_third_place_ratio\",\"trainer_place_ratio\",\"Win_Perc\",\"Place_Perc\"]\n",
    "\n",
    "string_cols = [\"Surface_Type\",\"Distance_Type\",\"Class\",\"Ranking\",\"Course\",\"Sex\"]\n",
    "string_cols_idx = [\"Surface_Type_index\",\"Distance_Type_index\",\"Class_index\",\"Ranking_index\",\"Course_index\",\"Sex_index\"]\n",
    "    \n",
    "StringIdxer = StringIndexer(inputCols=string_cols, outputCols=string_cols_idx)\n",
    "\n",
    "df_dataset2 = StringIdxer.fit(df_dataset).transform(df_dataset)\n",
    "df_dataset2.cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = df_dataset2.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now repartition the test data and break them down into 10 different files and write it to a csv file.\n",
    "testData = df_dataset2.repartition(10)#Remove directory in case we rerun it multiple times.\n",
    "\n",
    "# dbutils.fs.rm(\"/train_data/\",True)#Create a directory\n",
    "\n",
    "# testData.write.format(\"CSV\").option(\"header\",True).save(\"/train_data\")\n",
    "\n",
    "testData.repartition(10).write.csv(\"/home/gt/spark-3.0.3-bin-hadoop2.7/MSBD5003/train_data/temp1.csv\")\n",
    "testData.repartition(10).write.csv(\"/home/gt/spark-3.0.3-bin-hadoop2.7/MSBD5003/train_data/temp2.csv\")\n",
    "testData.repartition(10).write.csv(\"/home/gt/spark-3.0.3-bin-hadoop2.7/MSBD5003/train_data/temp3.csv\")\n",
    "testData.repartition(10).write.csv(\"/home/gt/spark-3.0.3-bin-hadoop2.7/MSBD5003/train_data/temp4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o9341.load.\n: java.lang.ClassNotFoundException: Failed to find data source: *.csv. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:689)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:195)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:243)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: *.csv.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:663)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:663)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:663)\n\t... 13 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_827/2768312630.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ssc = StreamingContext(sc, 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m                   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    416\u001b[0m                 raise ValueError(\"If the path is provided for stream, it needs to be a \" +\n\u001b[1;32m    417\u001b[0m                                  \"non-empty string. List of paths are not supported.\")\n\u001b[0;32m--> 418\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.3-bin-hadoop2.7/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o9341.load.\n: java.lang.ClassNotFoundException: Failed to find data source: *.csv. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:689)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:195)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:243)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: *.csv.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:663)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:663)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:663)\n\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "# ssc = StreamingContext(sc, 10)\n",
    "\n",
    "df_dataset = spark.readStream.format(\"csv\")\\\n",
    "                  .option(\"header\",True)\\\n",
    "                    .schema(schema)\\\n",
    "                    .load(\"/home/gt/spark-3.0.3-bin-hadoop2.7/MSBD5003/train_data\")\n",
    "\n",
    "# df_dataset_schema = df_dataset.schema\n",
    "\n",
    "# df_dataset_rdd = df_dataset.rdd\n",
    "\n",
    "# df_dataset_rdd_Queue = df_dataset_rdd.randomSplit([1,1,1,1,1], 123)\n",
    "\n",
    "# ssc = StreamingContext(sc, 60)\n",
    "\n",
    "# df_lines = ssc.queueStream(df_dataset_rdd_Queue)\n",
    "# df_lines = df_lines.map(lambda x: (x[0], x[1], x[2], x[3], x[4], x[5], x[6], x[7], x[8], x[9], x[10], x[11], x[12], x[13], x[14], x[15], x[16], x[17], x[18], x[19], x[20], x[21], x[22], x[23]))\n",
    "\n",
    "# df_dataset = spark.createDataFrame(df_lines, schema = df_dataset_schema)\n",
    "                                   \n",
    "\n",
    "\n",
    "# VectorAssm = VectorAssembler(inputCols=features_cols, outputCol='features')\n",
    "# df_vector_dataset = VectorAssm.transform(df_dataset2)\n",
    "\n",
    "# scaler = MinMaxScaler(inputCol = \"features\" , outputCol=\"features_scaled\")\n",
    "# df_vector_dataset_scaled = scaler.fit(df_vector_dataset).transform(df_vector_dataset)\n",
    "\n",
    "                                   \n",
    "        \n",
    "# # Split the data into training and test sets (30% held out for testing)\n",
    "# (trainingData, testData) = df_vector_dataset_scaled.randomSplit([0.9, 0.1])\n",
    "\n",
    "# # featureIndexer = VectorIndexer(inputCol = \"features\", outputCol=\"indexedFeatures\", maxCategories=40).fit(df_vector_dataset)\n",
    "# featureIndexer = VectorIndexer(inputCol = \"features_scaled\", outputCol=\"indexedFeatures\").fit(df_vector_dataset_scaled)\n",
    "\n",
    "# # Train a GBT model.\n",
    "# gbt = GBTRegressor(featuresCol=\"indexedFeatures\", labelCol=\"Target\", maxIter=20)\n",
    "# rf = RandomForestRegressor(featuresCol=\"indexedFeatures\", labelCol=\"Target\")\n",
    "\n",
    "# # Chain indexer and GBT in a Pipeline\n",
    "# pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
    "# pipeline_rf = Pipeline(stages=[featureIndexer, rf])\n",
    "\n",
    "# # Train model.  This also runs the indexer.\n",
    "# model = pipeline.fit(trainingData)\n",
    "\n",
    "# # model_rf = pipeline_rf.fit(trainingData)\n",
    "\n",
    "# # Make predictions.\n",
    "# predictions = model.transform(testData)\n",
    "\n",
    "# # predictions_rf = model_rf.transform(testData)\n",
    "\n",
    "# # # Select example rows to display.\n",
    "# # predictions.select(\"prediction\", \"Target\", \"features\").show(5)\n",
    "\n",
    "\n",
    "# # Select (prediction, true label) and compute test error\n",
    "# evaluator = RegressionEvaluator(\n",
    "#     labelCol=\"Target\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "\n",
    "# rmse = evaluator.evaluate(predictions)\n",
    "# print(\"GBT Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "# # rmse = evaluator.evaluate(predictions_rf)\n",
    "# # print(\"RF Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "def funct1(rdd):\n",
    "    yield rdd.count()\n",
    "\n",
    "# aggDF = df_dataset.count()\n",
    "\n",
    "\n",
    "df_dataset \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# query.awaitTermination()\n",
    "\n",
    "# ssc.start()\n",
    "# ssc.awaitTermination(25)\n",
    "# ssc.stop(False)\n",
    "# print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from flask import Flask, request\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# @app.route('/whateverYouWant', methods=['POST'])  #can set first param to '/'\n",
    "\n",
    "# def toyFunction():\n",
    "#     posted_data = sc.parallelize([request.get_data()])\n",
    "#     return str(posted_data.collect()[0])\n",
    "\n",
    "# if __name__ == '__main_':\n",
    "#     app.run(port=7000)    #note set to 8080!\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.py\n",
    "from flask import Flask\n",
    "from flask import Flask, render_template\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def prediction_results():\n",
    "    return render_template('main.html',Date='2021-12-18', Pred_date='2021-12-18')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x7f2afd750a90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'run.py' (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gt/.local/bin/flask\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/flask/cli.py\", line 994, in main\n",
      "    cli.main(args=sys.argv[1:])\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/flask/cli.py\", line 600, in main\n",
      "    return super().main(*args, **kwargs)\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/click/core.py\", line 1053, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/click/core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/click/core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/click/core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/click/decorators.py\", line 84, in new_func\n",
      "    return ctx.invoke(f, obj, *args, **kwargs)\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/click/core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/flask/cli.py\", line 853, in run_command\n",
      "    run_simple(\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/werkzeug/serving.py\", line 1010, in run_simple\n",
      "    inner()\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/werkzeug/serving.py\", line 950, in inner\n",
      "    srv = make_server(\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/werkzeug/serving.py\", line 782, in make_server\n",
      "    return ThreadedWSGIServer(\n",
      "  File \"/home/gt/.local/lib/python3.8/site-packages/werkzeug/serving.py\", line 688, in __init__\n",
      "    super().__init__(server_address, handler)  # type: ignore\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 452, in __init__\n",
      "    self.server_bind()\n",
      "  File \"/usr/lib/python3.8/http/server.py\", line 138, in server_bind\n",
      "    socketserver.TCPServer.server_bind(self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 466, in server_bind\n",
      "    self.socket.bind(self.server_address)\n",
      "OSError: [Errno 98] Address already in use\n"
     ]
    }
   ],
   "source": [
    "import subprocess as sp\n",
    "\n",
    "server = sp.Popen(\"FLASK_APP=run.py flask run\", shell=True)\n",
    "server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\\n<title>500 Internal Server Error</title>\\n<h1>Internal Server Error</h1>\\n<p>The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.</p>\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "requests.get(url=\"http://127.0.0.1:5000\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "server.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage and place percentage calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9610:=========================================>          (159 + 8) / 200]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55404\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Horse_ID=41, Race_ID=26159, Date=datetime.date(2021, 1, 10), Trainer_ID=14, Jockey_ID=5, Weight='132', Age_At_Race=4, Win_odds=4.199999809265137, Win_odds_%=0.19485487860215805, Draw='6', Place=3, Prize=1000000, Surface_Type='Turf', Distance_Type='Medium', Class='Class 4', Ranking='60-40', Course='Happy Valley', State='Active', Sex=' Gelding', First_Race_Date=datetime.date(2020, 1, 1), Finish_time_mille_second=10.14799976348877, Weather_Date=datetime.date(2021, 1, 10), Weather_Course='Happy Valley', MaxTemp=16.4, MinTemp=11.5, jockey_first_place_ratio=0.21294117647058824, jockey_second_place_ratio=0.1468627450980392, jockey_third_place_ratio=0.1203921568627451, jockey_place_ratio=0.4801960784313726, jockey_lose_ratio=0.5198039215686274, trainer_first_place_ratio=0.06744157888364209, trainer_second_place_ratio=0.0739799007143722, trainer_third_place_ratio=0.07434314081607943, trainer_place_ratio=0.21576462041409372, trainer_lose_ratio=0.7842353795859063, Win_Perc=0.0, Place_Perc=21.428571701049805)]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_records_jockey_trainer_win_place.rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####\n",
    "# Save the df_dataset to mongo\n",
    "####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9661:================================================>   (187 + 8) / 200]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| max(Date)|\n",
      "+----------+\n",
      "|2021-01-31|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9662:==========================================>         (165 + 9) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dataset.select(max('Date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_date = '1/1/2008'\n",
    "test_date = '1/10/2021'\n",
    "\n",
    "#########\n",
    "# Get data from mongo\n",
    "#########\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4305:=======================================>            (153 + 8) / 200]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+--------+-------------------+-----+-------+------------+-------------+-------+-------+------------+--------+-------+-------+------------------------+-------------------------+------------------------+------------------+-------------------------+--------------------------+-------------------------+-------------------+--------+----------+------+\n",
      "|      Date|Weight|Age_At_Race|Win_odds|         Win_odds_%| Draw|  Prize|Surface_Type|Distance_Type|  Class|Ranking|      Course|     Sex|MaxTemp|MinTemp|jockey_first_place_ratio|jockey_second_place_ratio|jockey_third_place_ratio|jockey_place_ratio|trainer_first_place_ratio|trainer_second_place_ratio|trainer_third_place_ratio|trainer_place_ratio|Win_Perc|Place_Perc|Target|\n",
      "+----------+------+-----------+--------+-------------------+-----+-------+------------+-------------+-------+-------+------------+--------+-------+-------+------------------------+-------------------------+------------------------+------------------+-------------------------+--------------------------+-------------------------+-------------------+--------+----------+------+\n",
      "|2021-01-10| 132.0|          4|     4.2|0.19485487860215805|132.0|1000000|        Turf|       Medium|Class 4|  60-40|Happy Valley| Gelding|   16.4|   11.5|     0.21294117647058824|       0.1468627450980392|      0.1203921568627451|0.4801960784313726|      0.06744157888364209|        0.0739799007143722|      0.07434314081607943|0.21576462041409372|     0.0| 21.428572|10.148|\n",
      "+----------+------+-----------+--------+-------------------+-----+-------+------------+-------------+-------+-------+------------+--------+-------+-------+------------------------+-------------------------+------------------------+------------------+-------------------------+--------------------------+-------------------------+-------------------+--------+----------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4305:==============================================>     (177 + 9) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dataset.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset.select('class_rank').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.select('Prize').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|Surface_Type|\n",
      "+------------+\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dataset.select('Surface_Type').where('Surface_Type is null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Distance_Type|\n",
      "+-------------+\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dataset.select('Distance_Type').where('Distance_Type is null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|Class|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dataset.select('Class').where('Class is null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2028"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.select('Ranking').where('Ranking is null').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Course|\n",
      "+------+\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dataset.select('Course').where('Course is null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|Sex|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dataset.select('Sex').where('Sex is null').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11517/669053973.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Ranking is not null and Sex is not null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_dataset' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53376"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cols = df_dataset.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset = df_dataset.withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler1 = MinMaxScaler(inputCol = \"Target\" , outputCol=\"Target_scaled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset_sacled = scaler1.fit(df_dataset).transform(df_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset_sacled = scaler.fit(df_vector_dataset).transform(df_vector_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset_sacled.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_cols =  [\"Weight\",\"Age_At_Race\",\"Win_odds\",\"Win_odds_%\",\"Draw\",\"Prize\",\"Surface_Type_index\",\"Distance_Type_index\",\n",
    "\"Class_index\",\"Ranking_index\",\"Course_index\",\"Sex_index\",\"MaxTemp\",\"MinTemp\",\"jockey_first_place_ratio\",\"jockey_second_place_ratio\",\n",
    "\"jockey_third_place_ratio\",\"jockey_place_ratio\",\"trainer_first_place_ratio\",\"trainer_second_place_ratio\",\n",
    "\"trainer_third_place_ratio\",\"trainer_place_ratio\",\"Win_Perc\",\"Place_Perc\"]\n",
    "\n",
    "string_cols = [\"Surface_Type\",\"Distance_Type\",\"Class\",\"Ranking\",\"Course\",\"Sex\"]\n",
    "string_cols_idx = [\"Surface_Type_index\",\"Distance_Type_index\",\"Class_index\",\"Ranking_index\",\"Course_index\",\"Sex_index\"]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_dataset2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# featureIndexer = VectorIndexer(inputCol = \"features\", outputCol=\"indexedFeatures\", maxCategories=40).fit(df_vector_dataset)\n",
    "featureIndexer = VectorIndexer(inputCol = \"features_scaled\", outputCol=\"indexedFeatures\").fit(df_vector_dataset_scaled)\n",
    "\n",
    "# Train a GBT model.\n",
    "\n",
    "\n",
    "# Chain indexer and GBT in a Pipeline\n",
    "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
    "\n",
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "# # Select example rows to display.\n",
    "# predictions.select(\"prediction\", \"Target\", \"features\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7240:============================================>       (173 + 8) / 200]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+--------+------------------+-----+------+------------+-------------+-------+-------+-------+--------+-------+-------+------------------------+-------------------------+------------------------+------------------+-------------------------+--------------------------+-------------------------+-------------------+--------+----------+------+---------+-----------+-------------+-------------------+------------------+------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|      Date|Weight|Age_At_Race|Win_odds|        Win_odds_%| Draw| Prize|Surface_Type|Distance_Type|  Class|Ranking| Course|     Sex|MaxTemp|MinTemp|jockey_first_place_ratio|jockey_second_place_ratio|jockey_third_place_ratio|jockey_place_ratio|trainer_first_place_ratio|trainer_second_place_ratio|trainer_third_place_ratio|trainer_place_ratio|Win_Perc|Place_Perc|Target|Sex_index|Class_index|Ranking_index|Distance_Type_index|Surface_Type_index|Course_index|            features|     features_scaled|     indexedFeatures|       prediction|\n",
      "+----------+------+-----------+--------+------------------+-----+------+------------+-------------+-------+-------+-------+--------+-------+-------+------------------------+-------------------------+------------------------+------------------+-------------------------+--------------------------+-------------------------+-------------------+--------+----------+------+---------+-----------+-------------+-------------------+------------------+------------+--------------------+--------------------+--------------------+-----------------+\n",
      "|2016-01-01| 129.0|          3|     3.5|0.2336541000001076|129.0|800000|        Turf|        Short|Class 4|  60-40|Sha Tin| Gelding|   21.8|   15.6|     0.21294117647058824|       0.1468627450980392|      0.1203921568627451|0.4801960784313726|      0.07550585729499468|       0.07667731629392971|      0.07433439829605963|0.22651757188498403|     0.0|       0.0| 8.311|      0.0|        0.0|          0.0|                0.0|               0.0|         0.0|[129.0,3.0,3.5,0....|[0.86666666666666...|[0.86666666666666...|7.287293282609338|\n",
      "+----------+------+-----------+--------+------------------+-----+------+------------+-------------+-------+-------+-------+--------+-------+-------+------------------------+-------------------------+------------------------+------------------+-------------------------+--------------------------+-------------------------+-------------------+--------+----------+------+---------+-----------+-------------+-------------------+------------------+------------+--------------------+--------------------+--------------------+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7240:===================================================>(199 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9499:=====================================>              (145 + 9) / 200]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.655814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9499:==============================================>     (179 + 8) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressionModel: uid=RandomForestRegressor_c50d8df844f9, numTrees=20, numFeatures=24\n"
     ]
    }
   ],
   "source": [
    "gbtModel = model.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27798/3641011136.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction' is not defined"
     ]
    }
   ],
   "source": [
    "prediction.select('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9523:===============================================>    (183 + 8) / 200]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| max(Date)|\n",
      "+----------+\n",
      "|2021-01-31|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9524:======================================>             (148 + 8) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_vector_dataset_scaled.select(max('Date')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
